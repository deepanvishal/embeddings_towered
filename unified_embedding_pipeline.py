import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import pickle
import json
from scipy.sparse import load_npz
from collections import defaultdict

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============================================================================
# CONFIGURATION
# ============================================================================

MIN_PARAMETER_RATIO = 10  # Minimum samples per parameter
OVERFITTING_THRESHOLD = 1.5  # Val/train loss ratio threshold
DROPOUT_RATE = 0.3
LEARNING_RATE = 0.001
EARLY_STOPPING_PATIENCE = 15
BATCH_SIZE = 64

# ============================================================================
# LOAD DATA
# ============================================================================

print("\nLoading data...")
proc_matrix = load_npz('procedure_vectors.npz')
all_pins = np.load('all_pins.npy', allow_pickle=True).tolist()

with open('pin_to_label.pkl', 'rb') as f:
    pin_to_label = pickle.load(f)

print(f"Total providers: {len(all_pins)}")
print(f"Labeled providers: {len(pin_to_label)}")
print(f"Unlabeled providers: {len(all_pins) - len(pin_to_label)}")
print(f"Procedure codes: {proc_matrix.shape[1]}")

# Filter to labeled providers for Phase 1 training
labeled_mask = [pin in pin_to_label for pin in all_pins]
labeled_indices = [i for i, is_labeled in enumerate(labeled_mask) if is_labeled]
proc_matrix_labeled = proc_matrix[labeled_indices]
all_pins_labeled = [all_pins[i] for i in labeled_indices]

proc_tensor_labeled = torch.FloatTensor(proc_matrix_labeled.toarray()).to(device)
proc_tensor_full = torch.FloatTensor(proc_matrix.toarray()).to(device)

input_dim = proc_tensor_labeled.shape[1]
n_labeled = len(all_pins_labeled)
n_total = len(all_pins)

print(f"Input dimension: {input_dim}")

# Create specialty mappings
unique_specialties = sorted(set(pin_to_label.values()))
specialty_to_id = {spec: idx for idx, spec in enumerate(unique_specialties)}
id_to_specialty = {idx: spec for spec, idx in specialty_to_id.items()}
num_specialties = len(unique_specialties)

pin_to_idx_labeled = {pin: idx for idx, pin in enumerate(all_pins_labeled)}
specialty_ids_labeled = torch.LongTensor([
    specialty_to_id[pin_to_label[pin]] for pin in all_pins_labeled
]).to(device)

print(f"Number of specialties: {num_specialties}")

# Specialty counts
specialty_counts = defaultdict(int)
for specialty in pin_to_label.values():
    specialty_counts[specialty] += 1

print("\nSpecialty distribution:")
for spec in sorted(specialty_counts.keys(), key=lambda x: specialty_counts[x], reverse=True):
    print(f"  {spec:25s}: {specialty_counts[spec]:4d}")

# Train/val/test split for labeled data
np.random.seed(42)
indices = list(range(n_labeled))
np.random.shuffle(indices)

n_train = int(0.7 * n_labeled)
n_val = int(0.15 * n_labeled)

train_indices = indices[:n_train]
val_indices = indices[n_train:n_train+n_val]
test_indices = indices[n_train+n_val:]

print(f"\nTrain: {len(train_indices)} ({100*len(train_indices)/n_labeled:.1f}%)")
print(f"Val:   {len(val_indices)} ({100*len(val_indices)/n_labeled:.1f}%)")
print(f"Test:  {len(test_indices)} ({100*len(test_indices)/n_labeled:.1f}%)")

# Compute specialty weights for balanced training
total_samples = len(pin_to_label)
specialty_weights = {}
for specialty, count in specialty_counts.items():
    spec_id = specialty_to_id[specialty]
    weight = total_samples / (num_specialties * count)
    specialty_weights[spec_id] = weight

min_weight = min(specialty_weights.values())
max_weight = 10 * min_weight

for spec_id in specialty_weights:
    specialty_weights[spec_id] = min(specialty_weights[spec_id], max_weight)

weight_tensor = torch.FloatTensor([specialty_weights[i] for i in range(num_specialties)]).to(device)

# Identify small specialties for Stage 1 training
small_specialties = [spec for spec, count in specialty_counts.items() if count < 100]
small_specialty_ids = [specialty_to_id[spec] for spec in small_specialties]

stage1_train_indices = []
for spec_id in small_specialty_ids:
    spec_indices = [idx for idx in train_indices if specialty_ids_labeled[idx] == spec_id]
    stage1_train_indices.extend(spec_indices)

large_specialty_ids = [sid for sid in range(num_specialties) if sid not in small_specialty_ids]
for spec_id in large_specialty_ids:
    spec_indices = [idx for idx in train_indices if specialty_ids_labeled[idx] == spec_id]
    sample_size = int(0.2 * len(spec_indices))
    stage1_train_indices.extend(np.random.choice(spec_indices, sample_size, replace=False).tolist())

print(f"\nStage 1 training samples: {len(stage1_train_indices)}")

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def calculate_model_parameters(input_dim, latent_dim, specialty_emb_dim=32):
    """Calculate total parameters for specialty-conditioned autoencoder"""
    enc_params = (input_dim + specialty_emb_dim) * 512 + 512 * 256 + 256 * latent_dim
    dec_params = (latent_dim + specialty_emb_dim) * 256 + 256 * 512 + 512 * input_dim
    specialty_emb_params = num_specialties * specialty_emb_dim
    total = enc_params + dec_params + specialty_emb_params
    return total

def calculate_reduction_parameters(input_dim, latent_dim):
    """Calculate parameters for reduction autoencoder"""
    enc_params = input_dim * 512 + 512 * latent_dim
    dec_params = latent_dim * 512 + 512 * input_dim
    return enc_params + dec_params

def check_overfitting(train_loss, val_loss, threshold=OVERFITTING_THRESHOLD):
    """Check if model is overfitting based on loss ratio"""
    ratio = val_loss / train_loss
    if ratio > threshold:
        return True, ratio
    return False, ratio

def masked_mse_loss(recon, x):
    """MSE loss only on non-zero elements"""
    mask = (x > 0).float()
    mse = ((recon - x) ** 2) * mask
    loss = mse.sum() / (mask.sum() + 1e-8)
    return loss

def weighted_loss(recon, x, specialty_ids, weight_tensor):
    """Weighted MSE loss by specialty frequency"""
    sample_losses = F.mse_loss(recon, x, reduction='none').mean(dim=1)
    batch_weights = weight_tensor[specialty_ids]
    return (sample_losses * batch_weights).mean()

class BalancedSpecialtyBatchSampler:
    """Sample equal number from each specialty per batch"""
    def __init__(self, indices, specialty_ids_tensor, num_specialties, batch_size):
        self.indices = indices
        self.specialty_ids = specialty_ids_tensor
        self.num_specialties = num_specialties
        self.batch_size = batch_size
        self.samples_per_specialty = max(1, batch_size // num_specialties)
        
        self.specialty_pools = defaultdict(list)
        for idx in indices:
            spec_id = int(self.specialty_ids[idx].item())
            self.specialty_pools[spec_id].append(idx)
        
        for spec_id in self.specialty_pools:
            np.random.shuffle(self.specialty_pools[spec_id])
        
        self.iterators = {spec_id: 0 for spec_id in range(num_specialties)}
    
    def __iter__(self):
        return self
    
    def __next__(self):
        batch = []
        for spec_id in range(self.num_specialties):
            if spec_id not in self.specialty_pools or len(self.specialty_pools[spec_id]) == 0:
                continue
            
            pool = self.specialty_pools[spec_id]
            start = self.iterators[spec_id]
            
            for _ in range(self.samples_per_specialty):
                if start >= len(pool):
                    np.random.shuffle(pool)
                    start = 0
                batch.append(pool[start])
                start += 1
            
            self.iterators[spec_id] = start
        
        if len(batch) == 0:
            raise StopIteration
        
        np.random.shuffle(batch)
        return batch

# ============================================================================
# PHASE 1: SPECIALTY-CONDITIONED AUTOENCODER
# ============================================================================

print("\n" + "="*80)
print("PHASE 1: LATENT DIMENSION SEARCH")
print("="*80)

class SpecialtyConditionedAutoencoder(nn.Module):
    def __init__(self, input_dim, num_specialties, latent_dim, specialty_emb_dim=32):
        super().__init__()
        
        self.specialty_embedding = nn.Embedding(num_specialties, specialty_emb_dim)
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim + specialty_emb_dim, 512),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(256, latent_dim)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim + specialty_emb_dim, 256),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(512, input_dim)
        )
    
    def encode(self, x, specialty_id):
        spec_emb = self.specialty_embedding(specialty_id)
        encoder_input = torch.cat([x, spec_emb], dim=1)
        return self.encoder(encoder_input)
    
    def decode(self, z, specialty_id):
        spec_emb = self.specialty_embedding(specialty_id)
        decoder_input = torch.cat([z, spec_emb], dim=1)
        return self.decoder(decoder_input)
    
    def forward(self, x, specialty_id):
        z = self.encode(x, specialty_id)
        recon = self.decode(z, specialty_id)
        return recon, z

# Find safe latent dimensions
latent_dim_candidates = [32, 64, 128, 256]
safe_latent_dims = []

print(f"\nChecking parameter ratios (min required: 1:{MIN_PARAMETER_RATIO}):")
for latent_dim in latent_dim_candidates:
    params = calculate_model_parameters(input_dim, latent_dim)
    ratio = params / len(train_indices)
    
    print(f"  Latent dim {latent_dim:3d}: {params:,} params, ratio 1:{ratio:.1f}", end="")
    
    if ratio <= MIN_PARAMETER_RATIO:
        safe_latent_dims.append(latent_dim)
        print(" [SAFE]")
    else:
        print(" [RISKY - SKIPPED]")

if len(safe_latent_dims) == 0:
    print("\nWARNING: No safe latent dimensions found with standard architecture.")
    print("Using smallest option (32) with extreme regularization.")
    safe_latent_dims = [32]

print(f"\nTesting latent dimensions: {safe_latent_dims}")

# Quick evaluation of each safe latent dimension
def quick_eval_latent_dim(latent_dim, epochs=20):
    """Quick training to evaluate latent dimension"""
    model = SpecialtyConditionedAutoencoder(input_dim, num_specialties, latent_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        model.train()
        batch_losses = []
        
        n_batches = len(train_indices) // BATCH_SIZE
        for i in range(n_batches):
            batch_idx = train_indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
            batch_x = proc_tensor_labeled[batch_idx]
            batch_spec = specialty_ids_labeled[batch_idx]
            
            recon, z = model(batch_x, batch_spec)
            loss = masked_mse_loss(recon, batch_x)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            batch_losses.append(loss.item())
        
        model.eval()
        with torch.no_grad():
            val_batch_losses = []
            n_val_batches = len(val_indices) // BATCH_SIZE
            for i in range(n_val_batches):
                batch_idx = val_indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
                batch_x = proc_tensor_labeled[batch_idx]
                batch_spec = specialty_ids_labeled[batch_idx]
                
                recon, z = model(batch_x, batch_spec)
                loss = masked_mse_loss(recon, batch_x)
                val_batch_losses.append(loss.item())
        
        train_loss = np.mean(batch_losses)
        val_loss = np.mean(val_batch_losses)
        train_losses.append(train_loss)
        val_losses.append(val_loss)
    
    final_train = train_losses[-1]
    final_val = val_losses[-1]
    best_val = min(val_losses)
    overfit_ratio = final_val / final_train
    
    return {
        'latent_dim': latent_dim,
        'final_train': final_train,
        'final_val': final_val,
        'best_val': best_val,
        'overfit_ratio': overfit_ratio
    }

search_results = []
for latent_dim in safe_latent_dims:
    print(f"\nEvaluating latent_dim={latent_dim}...")
    result = quick_eval_latent_dim(latent_dim, epochs=20)
    search_results.append(result)
    
    is_overfitting, ratio = check_overfitting(result['final_train'], result['final_val'])
    status = "OVERFITTING WARNING" if is_overfitting else "OK"
    
    print(f"  Train: {result['final_train']:.4f}")
    print(f"  Val:   {result['final_val']:.4f}")
    print(f"  Ratio: {ratio:.3f} [{status}]")

# Select best latent dimension
valid_results = [r for r in search_results if r['overfit_ratio'] < OVERFITTING_THRESHOLD]
if len(valid_results) == 0:
    print("\nWARNING: All configurations show overfitting. Using lowest val loss.")
    best_result = min(search_results, key=lambda x: x['final_val'])
else:
    best_result = min(valid_results, key=lambda x: x['final_val'])

PHASE1_LATENT_DIM = best_result['latent_dim']

print(f"\n" + "="*80)
print(f"SELECTED LATENT DIM: {PHASE1_LATENT_DIM}")
print(f"  Val loss: {best_result['final_val']:.4f}")
print(f"  Overfit ratio: {best_result['overfit_ratio']:.3f}")
print("="*80)

with open('phase1_latent_dim_search.json', 'w') as f:
    json.dump(search_results, f, indent=2)

# ============================================================================
# PHASE 1: FULL TRAINING WITH SELECTED LATENT DIM
# ============================================================================

print("\n" + "="*80)
print("PHASE 1: TRAINING SPECIALTY-CONDITIONED AUTOENCODER")
print("="*80)
print(f"Latent dimension: {PHASE1_LATENT_DIM}")

model = SpecialtyConditionedAutoencoder(input_dim, num_specialties, PHASE1_LATENT_DIM).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

best_val_loss = float('inf')
patience_counter = 0
history = {'stage1_train': [], 'stage1_val': [], 'stage2_train': [], 'stage2_val': []}

# Stage 1: Small specialty focus
print("\nStage 1: Small specialty focus (30 epochs)")
for epoch in range(30):
    model.train()
    train_losses = []
    
    np.random.shuffle(stage1_train_indices)
    n_batches = len(stage1_train_indices) // BATCH_SIZE
    
    for i in range(n_batches):
        batch_idx = stage1_train_indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
        batch_x = proc_tensor_labeled[batch_idx]
        batch_spec = specialty_ids_labeled[batch_idx]
        
        recon, z = model(batch_x, batch_spec)
        loss = weighted_loss(recon, batch_x, batch_spec, weight_tensor)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_losses.append(loss.item())
    
    model.eval()
    with torch.no_grad():
        val_batch_losses = []
        n_val_batches = len(val_indices) // BATCH_SIZE
        for i in range(n_val_batches):
            batch_idx = val_indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
            batch_x = proc_tensor_labeled[batch_idx]
            batch_spec = specialty_ids_labeled[batch_idx]
            
            recon, z = model(batch_x, batch_spec)
            loss = masked_mse_loss(recon, batch_x)
            val_batch_losses.append(loss.item())
    
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_batch_losses)
    
    history['stage1_train'].append(train_loss)
    history['stage1_val'].append(val_loss)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        best_model_state = model.state_dict().copy()
    else:
        patience_counter += 1
    
    is_overfitting, ratio = check_overfitting(train_loss, val_loss)
    status = " [OVERFITTING WARNING]" if is_overfitting else ""
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d}: Train={train_loss:.4f}, Val={val_loss:.4f}, "
              f"Best={best_val_loss:.4f}, Ratio={ratio:.3f}{status}")
    
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"Early stopping at epoch {epoch+1}")
        break

model.load_state_dict(best_model_state)

# Stage 2: Balanced training
print("\nStage 2: Balanced training (70 epochs)")
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE * 0.5)

balanced_sampler = BalancedSpecialtyBatchSampler(
    train_indices, specialty_ids_labeled, num_specialties, BATCH_SIZE
)

best_val_loss = float('inf')
patience_counter = 0

for epoch in range(70):
    model.train()
    train_losses = []
    
    sampler_iter = iter(balanced_sampler)
    for _ in range(len(train_indices) // BATCH_SIZE):
        try:
            batch_idx = next(sampler_iter)
        except StopIteration:
            break
        
        batch_x = proc_tensor_labeled[batch_idx]
        batch_spec = specialty_ids_labeled[batch_idx]
        
        recon, z = model(batch_x, batch_spec)
        loss = weighted_loss(recon, batch_x, batch_spec, weight_tensor)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_losses.append(loss.item())
    
    model.eval()
    with torch.no_grad():
        val_batch_losses = []
        n_val_batches = len(val_indices) // BATCH_SIZE
        for i in range(n_val_batches):
            batch_idx = val_indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
            batch_x = proc_tensor_labeled[batch_idx]
            batch_spec = specialty_ids_labeled[batch_idx]
            
            recon, z = model(batch_x, batch_spec)
            loss = masked_mse_loss(recon, batch_x)
            val_batch_losses.append(loss.item())
    
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_batch_losses)
    
    history['stage2_train'].append(train_loss)
    history['stage2_val'].append(val_loss)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        best_model_state = model.state_dict().copy()
    else:
        patience_counter += 1
    
    is_overfitting, ratio = check_overfitting(train_loss, val_loss)
    status = " [OVERFITTING WARNING]" if is_overfitting else ""
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d}: Train={train_loss:.4f}, Val={val_loss:.4f}, "
              f"Best={best_val_loss:.4f}, Ratio={ratio:.3f}{status}")
    
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"Early stopping at epoch {epoch+1}")
        break

model.load_state_dict(best_model_state)

# Final evaluation
model.eval()
with torch.no_grad():
    test_batch_losses = []
    n_test_batches = len(test_indices) // BATCH_SIZE
    for i in range(n_test_batches):
        batch_idx = test_indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
        batch_x = proc_tensor_labeled[batch_idx]
        batch_spec = specialty_ids_labeled[batch_idx]
        
        recon, z = model(batch_x, batch_spec)
        loss = masked_mse_loss(recon, batch_x)
        test_batch_losses.append(loss.item())
    
    test_loss = np.mean(test_batch_losses)

final_train = history['stage2_train'][-1]
final_val = history['stage2_val'][-1]

print(f"\nPhase 1 Complete:")
print(f"  Train loss: {final_train:.4f}")
print(f"  Val loss:   {final_val:.4f}")
print(f"  Test loss:  {test_loss:.4f}")

is_overfitting_val, val_ratio = check_overfitting(final_train, final_val)
is_overfitting_test, test_ratio = check_overfitting(final_train, test_loss)

print(f"\nOverfitting check:")
print(f"  Val/Train:  {val_ratio:.3f} {'[OVERFITTING WARNING]' if is_overfitting_val else '[OK]'}")
print(f"  Test/Train: {test_ratio:.3f} {'[OVERFITTING WARNING]' if is_overfitting_test else '[OK]'}")

torch.save(model.state_dict(), 'phase1_specialty_conditioned_autoencoder.pth')

metadata = {
    'latent_dim': PHASE1_LATENT_DIM,
    'input_dim': input_dim,
    'num_specialties': num_specialties,
    'specialty_to_id': specialty_to_id,
    'id_to_specialty': id_to_specialty,
    'final_train_loss': final_train,
    'final_val_loss': final_val,
    'final_test_loss': test_loss,
    'val_train_ratio': val_ratio,
    'test_train_ratio': test_ratio,
    'latent_dim_search': search_results
}

with open('phase1_metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print("\nSaved: phase1_specialty_conditioned_autoencoder.pth")
print("Saved: phase1_metadata.pkl")

# ============================================================================
# PHASE 2: GENERATE MULTI-VIEW EMBEDDINGS
# ============================================================================

print("\n" + "="*80)
print("PHASE 2: GENERATING MULTI-VIEW EMBEDDINGS")
print("="*80)

multiview_dim = num_specialties * PHASE1_LATENT_DIM
print(f"Multi-view dimension: {num_specialties} specialties × {PHASE1_LATENT_DIM} = {multiview_dim}")

model.eval()
all_multiview = []

batch_size_inference = 256

with torch.no_grad():
    for i in range(0, n_total, batch_size_inference):
        end_idx = min(i + batch_size_inference, n_total)
        batch_x = proc_tensor_full[i:end_idx]
        
        batch_multiview = []
        for spec_id in range(num_specialties):
            spec_ids = torch.full((end_idx - i,), spec_id, dtype=torch.long, device=device)
            embeddings = model.encode(batch_x, spec_ids)
            batch_multiview.append(embeddings.cpu().numpy())
        
        combined = np.concatenate(batch_multiview, axis=1)
        all_multiview.append(combined)
        
        if (i // batch_size_inference) % 10 == 0:
            print(f"  Encoded {end_idx}/{n_total} providers...")

multiview_embeddings = np.vstack(all_multiview)
print(f"\nMulti-view embeddings shape: {multiview_embeddings.shape}")

np.save('phase2_multiview_embeddings.npy', multiview_embeddings)
print("Saved: phase2_multiview_embeddings.npy")

# ============================================================================
# PHASE 3A: REDUCTION AUTOENCODER - LATENT DIM SEARCH
# ============================================================================

print("\n" + "="*80)
print("PHASE 3A: REDUCTION AUTOENCODER - LATENT DIM SEARCH")
print("="*80)

# Split multiview data for reduction autoencoder training
n_reduction_train = int(0.8 * n_total)
reduction_train_indices = list(range(n_reduction_train))
reduction_val_indices = list(range(n_reduction_train, n_total))

print(f"Reduction train samples: {len(reduction_train_indices)}")
print(f"Reduction val samples: {len(reduction_val_indices)}")

class ReductionAutoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(512, latent_dim)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Dropout(DROPOUT_RATE),
            nn.Linear(512, input_dim)
        )
    
    def forward(self, x):
        z = self.encoder(x)
        recon = self.decoder(z)
        return recon, z

# Find safe reduction latent dimensions
reduction_latent_candidates = [128, 256, 512]
safe_reduction_latents = []

print(f"\nChecking parameter ratios for reduction autoencoder:")
for latent_dim in reduction_latent_candidates:
    params = calculate_reduction_parameters(multiview_dim, latent_dim)
    ratio = params / len(reduction_train_indices)
    
    print(f"  Latent dim {latent_dim:3d}: {params:,} params, ratio 1:{ratio:.1f}", end="")
    
    if ratio <= MIN_PARAMETER_RATIO:
        safe_reduction_latents.append(latent_dim)
        print(" [SAFE]")
    else:
        print(" [RISKY - SKIPPED]")

if len(safe_reduction_latents) == 0:
    print("\nWARNING: No safe reduction latent dimensions found.")
    print("Using 256 with extreme regularization.")
    safe_reduction_latents = [256]

print(f"\nTesting reduction latent dimensions: {safe_reduction_latents}")

multiview_tensor = torch.FloatTensor(multiview_embeddings).to(device)

def quick_eval_reduction_latent(latent_dim, epochs=20):
    """Quick evaluation of reduction autoencoder latent dimension"""
    model = ReductionAutoencoder(multiview_dim, latent_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        model.train()
        batch_losses = []
        
        n_batches = len(reduction_train_indices) // BATCH_SIZE
        for i in range(n_batches):
            start = i * BATCH_SIZE
            end = min(start + BATCH_SIZE, len(reduction_train_indices))
            batch_idx = reduction_train_indices[start:end]
            batch_x = multiview_tensor[batch_idx]
            
            recon, z = model(batch_x)
            loss = F.mse_loss(recon, batch_x)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            batch_losses.append(loss.item())
        
        model.eval()
        with torch.no_grad():
            val_batch_losses = []
            n_val_batches = len(reduction_val_indices) // BATCH_SIZE
            for i in range(n_val_batches):
                start = i * BATCH_SIZE
                end = min(start + BATCH_SIZE, len(reduction_val_indices))
                batch_idx = reduction_val_indices[start:end]
                batch_x = multiview_tensor[batch_idx]
                
                recon, z = model(batch_x)
                loss = F.mse_loss(recon, batch_x)
                val_batch_losses.append(loss.item())
        
        train_loss = np.mean(batch_losses)
        val_loss = np.mean(val_batch_losses)
        train_losses.append(train_loss)
        val_losses.append(val_loss)
    
    final_train = train_losses[-1]
    final_val = val_losses[-1]
    best_val = min(val_losses)
    overfit_ratio = final_val / final_train
    
    return {
        'latent_dim': latent_dim,
        'final_train': final_train,
        'final_val': final_val,
        'best_val': best_val,
        'overfit_ratio': overfit_ratio
    }

reduction_search_results = []
for latent_dim in safe_reduction_latents:
    print(f"\nEvaluating reduction latent_dim={latent_dim}...")
    result = quick_eval_reduction_latent(latent_dim, epochs=20)
    reduction_search_results.append(result)
    
    is_overfitting, ratio = check_overfitting(result['final_train'], result['final_val'])
    status = "OVERFITTING WARNING" if is_overfitting else "OK"
    
    print(f"  Train: {result['final_train']:.4f}")
    print(f"  Val:   {result['final_val']:.4f}")
    print(f"  Ratio: {ratio:.3f} [{status}]")

# Select best reduction latent dimension (prefer larger if safe)
valid_reduction_results = [r for r in reduction_search_results if r['overfit_ratio'] < OVERFITTING_THRESHOLD]
if len(valid_reduction_results) == 0:
    print("\nWARNING: All reduction configurations show overfitting.")
    best_reduction_result = min(reduction_search_results, key=lambda x: x['final_val'])
else:
    # Among valid options, prefer larger latent dimension (preserves more information)
    best_reduction_result = max(valid_reduction_results, key=lambda x: x['latent_dim'])

REDUCTION_LATENT_DIM = best_reduction_result['latent_dim']

print(f"\n" + "="*80)
print(f"SELECTED REDUCTION LATENT DIM: {REDUCTION_LATENT_DIM}")
print(f"  Val loss: {best_reduction_result['final_val']:.4f}")
print(f"  Overfit ratio: {best_reduction_result['overfit_ratio']:.3f}")
print("="*80)

with open('phase3a_reduction_search.json', 'w') as f:
    json.dump(reduction_search_results, f, indent=2)

# ============================================================================
# PHASE 3A: FULL TRAINING OF REDUCTION AUTOENCODER
# ============================================================================

print("\n" + "="*80)
print("PHASE 3A: TRAINING REDUCTION AUTOENCODER")
print("="*80)
print(f"Input: {multiview_dim} → Output: {REDUCTION_LATENT_DIM}")

reduction_model = ReductionAutoencoder(multiview_dim, REDUCTION_LATENT_DIM).to(device)
optimizer = torch.optim.Adam(reduction_model.parameters(), lr=LEARNING_RATE)

best_val_loss = float('inf')
patience_counter = 0
reduction_history = {'train': [], 'val': []}

for epoch in range(100):
    reduction_model.train()
    train_losses = []
    
    n_batches = len(reduction_train_indices) // BATCH_SIZE
    for i in range(n_batches):
        start = i * BATCH_SIZE
        end = min(start + BATCH_SIZE, len(reduction_train_indices))
        batch_idx = reduction_train_indices[start:end]
        batch_x = multiview_tensor[batch_idx]
        
        recon, z = reduction_model(batch_x)
        loss = F.mse_loss(recon, batch_x)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(reduction_model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_losses.append(loss.item())
    
    reduction_model.eval()
    with torch.no_grad():
        val_batch_losses = []
        n_val_batches = len(reduction_val_indices) // BATCH_SIZE
        for i in range(n_val_batches):
            start = i * BATCH_SIZE
            end = min(start + BATCH_SIZE, len(reduction_val_indices))
            batch_idx = reduction_val_indices[start:end]
            batch_x = multiview_tensor[batch_idx]
            
            recon, z = reduction_model(batch_x)
            loss = F.mse_loss(recon, batch_x)
            val_batch_losses.append(loss.item())
    
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_batch_losses)
    
    reduction_history['train'].append(train_loss)
    reduction_history['val'].append(val_loss)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        best_reduction_state = reduction_model.state_dict().copy()
    else:
        patience_counter += 1
    
    is_overfitting, ratio = check_overfitting(train_loss, val_loss)
    status = " [OVERFITTING WARNING]" if is_overfitting else ""
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d}: Train={train_loss:.4f}, Val={val_loss:.4f}, "
              f"Best={best_val_loss:.4f}, Ratio={ratio:.3f}{status}")
    
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"Early stopping at epoch {epoch+1}")
        break

reduction_model.load_state_dict(best_reduction_state)

print(f"\nPhase 3A Complete:")
print(f"  Best val loss: {best_val_loss:.4f}")

torch.save(reduction_model.state_dict(), 'phase3a_reduction_autoencoder.pth')

# Generate reduced embeddings for all providers
reduction_model.eval()
reduced_embeddings = []

with torch.no_grad():
    for i in range(0, n_total, batch_size_inference):
        end_idx = min(i + batch_size_inference, n_total)
        batch_x = multiview_tensor[i:end_idx]
        _, z = reduction_model(batch_x)
        reduced_embeddings.append(z.cpu().numpy())

reduced_embeddings = np.vstack(reduced_embeddings)
print(f"Reduced embeddings shape: {reduced_embeddings.shape}")

np.save('phase3a_reduced_embeddings.npy', reduced_embeddings)
print("Saved: phase3a_reduced_embeddings.npy")
print("Saved: phase3a_reduction_autoencoder.pth")

# ============================================================================
# PHASE 3B: SUPERVISED COMPRESSION NETWORK
# ============================================================================

print("\n" + "="*80)
print("PHASE 3B: SUPERVISED COMPRESSION NETWORK")
print("="*80)

FINAL_DIM = 128

class CompressionNetwork(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.Dropout(0.5)
        )
    
    def forward(self, x):
        return self.network(x)

def supervised_contrastive_loss(embeddings, labels, temperature=0.1):
    """Supervised contrastive loss"""
    device = embeddings.device
    batch_size = embeddings.shape[0]
    
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    labels = labels.contiguous().view(-1, 1)
    mask = torch.eq(labels, labels.T).float().to(device)
    
    similarity_matrix = torch.matmul(embeddings, embeddings.T)
    similarity_matrix = similarity_matrix / temperature
    
    logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)
    logits = similarity_matrix - logits_max.detach()
    
    logits_mask = torch.scatter(
        torch.ones_like(mask),
        1,
        torch.arange(batch_size).view(-1, 1).to(device),
        0
    )
    mask = mask * logits_mask
    
    exp_logits = torch.exp(logits) * logits_mask
    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)
    
    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)
    
    loss = -mean_log_prob_pos
    loss = loss.mean()
    
    return loss

# Map labeled providers to reduced embeddings
reduced_tensor = torch.FloatTensor(reduced_embeddings).to(device)

# Use same train/val/test split from Phase 1
labeled_to_full_idx = {all_pins_labeled[i]: i for i in range(n_labeled)}
full_idx_map = {all_pins[i]: i for i in range(n_total)}

train_full_indices = [full_idx_map[all_pins_labeled[i]] for i in train_indices]
val_full_indices = [full_idx_map[all_pins_labeled[i]] for i in val_indices]
test_full_indices = [full_idx_map[all_pins_labeled[i]] for i in test_indices]

print(f"Compression training samples: {len(train_full_indices)}")
print(f"Input dimension: {REDUCTION_LATENT_DIM}")
print(f"Output dimension: {FINAL_DIM}")

compression_params = REDUCTION_LATENT_DIM * FINAL_DIM
compression_ratio = compression_params / len(train_full_indices)
print(f"Parameters: {compression_params:,}")
print(f"Ratio: 1:{compression_ratio:.1f} {'[SAFE]' if compression_ratio <= MIN_PARAMETER_RATIO else '[RISKY]'}")

compression_model = CompressionNetwork(REDUCTION_LATENT_DIM, FINAL_DIM).to(device)
optimizer = torch.optim.AdamW(compression_model.parameters(), lr=0.0005, weight_decay=1e-4)

best_val_loss = float('inf')
patience_counter = 0
compression_history = {'train': [], 'val': []}

for epoch in range(50):
    compression_model.train()
    train_losses = []
    
    np.random.shuffle(train_full_indices)
    n_batches = len(train_full_indices) // BATCH_SIZE
    
    for i in range(n_batches):
        start = i * BATCH_SIZE
        end = min(start + BATCH_SIZE, len(train_full_indices))
        batch_idx = train_full_indices[start:end]
        
        batch_x = reduced_tensor[batch_idx]
        
        # Get labels for this batch
        batch_labels = []
        for idx in batch_idx:
            pin = all_pins[idx]
            batch_labels.append(specialty_to_id[pin_to_label[pin]])
        batch_labels = torch.LongTensor(batch_labels).to(device)
        
        compressed = compression_model(batch_x)
        loss = supervised_contrastive_loss(compressed, batch_labels, temperature=0.1)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(compression_model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_losses.append(loss.item())
    
    compression_model.eval()
    with torch.no_grad():
        val_batch_losses = []
        n_val_batches = len(val_full_indices) // BATCH_SIZE
        
        for i in range(n_val_batches):
            start = i * BATCH_SIZE
            end = min(start + BATCH_SIZE, len(val_full_indices))
            batch_idx = val_full_indices[start:end]
            
            batch_x = reduced_tensor[batch_idx]
            
            batch_labels = []
            for idx in batch_idx:
                pin = all_pins[idx]
                batch_labels.append(specialty_to_id[pin_to_label[pin]])
            batch_labels = torch.LongTensor(batch_labels).to(device)
            
            compressed = compression_model(batch_x)
            loss = supervised_contrastive_loss(compressed, batch_labels, temperature=0.1)
            val_batch_losses.append(loss.item())
    
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_batch_losses)
    
    compression_history['train'].append(train_loss)
    compression_history['val'].append(val_loss)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        best_compression_state = compression_model.state_dict().copy()
    else:
        patience_counter += 1
    
    is_overfitting, ratio = check_overfitting(train_loss, val_loss)
    status = " [OVERFITTING WARNING]" if is_overfitting else ""
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d}: Train={train_loss:.4f}, Val={val_loss:.4f}, "
              f"Best={best_val_loss:.4f}, Ratio={ratio:.3f}{status}")
    
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"Early stopping at epoch {epoch+1}")
        break

compression_model.load_state_dict(best_compression_state)

# Test evaluation
compression_model.eval()
with torch.no_grad():
    test_batch_losses = []
    n_test_batches = len(test_full_indices) // BATCH_SIZE
    
    for i in range(n_test_batches):
        start = i * BATCH_SIZE
        end = min(start + BATCH_SIZE, len(test_full_indices))
        batch_idx = test_full_indices[start:end]
        
        batch_x = reduced_tensor[batch_idx]
        
        batch_labels = []
        for idx in batch_idx:
            pin = all_pins[idx]
            batch_labels.append(specialty_to_id[pin_to_label[pin]])
        batch_labels = torch.LongTensor(batch_labels).to(device)
        
        compressed = compression_model(batch_x)
        loss = supervised_contrastive_loss(compressed, batch_labels, temperature=0.1)
        test_batch_losses.append(loss.item())
    
    test_loss = np.mean(test_batch_losses)

final_train = compression_history['train'][-1]
final_val = compression_history['val'][-1]

print(f"\nPhase 3B Complete:")
print(f"  Train loss: {final_train:.4f}")
print(f"  Val loss:   {final_val:.4f}")
print(f"  Test loss:  {test_loss:.4f}")

is_overfitting_val, val_ratio = check_overfitting(final_train, final_val)
is_overfitting_test, test_ratio = check_overfitting(final_train, test_loss)

print(f"\nOverfitting check:")
print(f"  Val/Train:  {val_ratio:.3f} {'[OVERFITTING WARNING]' if is_overfitting_val else '[OK]'}")
print(f"  Test/Train: {test_ratio:.3f} {'[OVERFITTING WARNING]' if is_overfitting_test else '[OK]'}")

torch.save(compression_model.state_dict(), 'phase3b_compression_network.pth')

# ============================================================================
# FINAL: GENERATE ALL EMBEDDINGS
# ============================================================================

print("\n" + "="*80)
print("GENERATING FINAL EMBEDDINGS FOR ALL PROVIDERS")
print("="*80)

compression_model.eval()
final_embeddings = []

with torch.no_grad():
    for i in range(0, n_total, batch_size_inference):
        end_idx = min(i + batch_size_inference, n_total)
        batch_x = reduced_tensor[i:end_idx]
        
        compressed = compression_model(batch_x)
        compressed_norm = F.normalize(compressed, p=2, dim=1)
        
        final_embeddings.append(compressed_norm.cpu().numpy())

final_embeddings = np.vstack(final_embeddings)
print(f"Final embeddings shape: {final_embeddings.shape}")

np.save('final_embeddings_128d.npy', final_embeddings)

# Create dataframe
embedding_data = {'PIN': all_pins}
for i in range(FINAL_DIM):
    embedding_data[f'emb_{i}'] = final_embeddings[:, i]

embeddings_df = pd.DataFrame(embedding_data)
embeddings_df.to_parquet('final_embeddings_128d.parquet', index=False)

print("\nSaved: final_embeddings_128d.npy")
print("Saved: final_embeddings_128d.parquet")

# Save final metadata
final_metadata = {
    'pipeline': 'unified_specialty_conditioned',
    'phase1_latent_dim': PHASE1_LATENT_DIM,
    'phase2_multiview_dim': multiview_dim,
    'phase3a_reduction_dim': REDUCTION_LATENT_DIM,
    'phase3b_final_dim': FINAL_DIM,
    'total_providers': n_total,
    'labeled_providers': n_labeled,
    'num_specialties': num_specialties,
    'specialty_to_id': specialty_to_id,
    'phase1_val_train_ratio': val_ratio,
    'phase1_test_train_ratio': test_ratio,
    'phase3b_val_train_ratio': compression_history['val'][-1] / compression_history['train'][-1],
    'phase3b_test_train_ratio': test_loss / compression_history['train'][-1]
}

with open('final_pipeline_metadata.pkl', 'wb') as f:
    pickle.dump(final_metadata, f)

print("Saved: final_pipeline_metadata.pkl")

print("\n" + "="*80)
print("PIPELINE COMPLETE")
print("="*80)
print(f"\nFinal outputs:")
print(f"  1. final_embeddings_128d.npy - Embeddings for all {n_total} providers")
print(f"  2. final_embeddings_128d.parquet - Embeddings with PIN column")
print(f"  3. final_pipeline_metadata.pkl - Complete metadata")
print(f"\nPipeline stages:")
print(f"  Phase 1: {input_dim} → {PHASE1_LATENT_DIM} (specialty-conditioned)")
print(f"  Phase 2: {PHASE1_LATENT_DIM} × {num_specialties} = {multiview_dim} (multi-view)")
print(f"  Phase 3A: {multiview_dim} → {REDUCTION_LATENT_DIM} (reduction autoencoder)")
print(f"  Phase 3B: {REDUCTION_LATENT_DIM} → {FINAL_DIM} (supervised compression)")
